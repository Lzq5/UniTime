<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <style type="text/css">
    
    body {
      font-family: "Times New Roman", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-weight: 200;
      font-size: 16px;
      margin-left: auto;
      margin-right: auto;
      width: 800px;
    }

    h1, h2{
      font-weight: 400;
      margin-top: 1.0em;
      margin-bottom: 1.0em;
    }

    h3{
      font-weight: 400;
      margin-top: 0.5em;
      margin-bottom: 0.5em;
    }

    p {
      font-size: 18px;
      font-weight: 200;
      line-height: 1.4;
    }

    code {
      font-size: 0.8rem;
      margin: 0 0.2rem;
      padding: 0.5rem 0.8rem;
      white-space: nowrap;
      background: #efefef;
      border: 1px solid #d3d3d3;
      color: #000000;
      border-radius: 3px;
    }

    pre>code {
      display: block;
      white-space: pre;
      line-height: 1.5;
      padding: 0;
      margin: 0;
    }

    pre.prettyprint>code {
      border: none;
    }

    .container {
      display: flex;
      align-items: center;
      justify-content: center
    }

    .image {
      flex-basis: 40%
    }

    .text {
      padding-left: 20px;
      padding-right: 20px;
    }

    .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
      padding: 20px;
    }

    video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.rounded {
      border: 0px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;

    }

    a:link,
    a:visited {
      color: #1367a7;
      text-decoration: none;
    }

    a:hover {
      color: #208799;
    }

    td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
    }

    hr {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    .abstract-container {
      background-color: #f8f9fa;
      padding: 0.5em;
      border-radius: 5px;
      margin: 0.5em 0;
    }

  </style>

<link rel="icon" href="./resources/icon.png">
  <title>UniTime</title>
</head>

<body data-new-gr-c-s-check-loaded="14.1093.0" data-gr-ext-installed="">
  <center>
    <span style="font-size:36px; color:#456b32; font-family: Varela Round,sans-serif; font-weight: 700; line-height: 65px;">
        UniTi<img src="./resources/icon.png" style="height: 1.0em; vertical-align: middle; margin-left: -0.05em; margin-right: -0.05em;">e
    </span>
    <span style="font-size:28px">: Universal Video Temporal Grounding</span>
    <br>
    <span style="font-size:28px">with Generative Multi-modal Large Language Models</span>
    <br>
    <br>
  </center>

  <table align="center" width="840px">
    <tbody>
      <tr>
        <td align="center" width="105px">
          <center> <span style="font-size:20px"><a href="https://github.com/Lzq5">Zeqian Li</a><sup>1</sup></span> </center>
        </td>
        <td align="center" width="100px">
          <center> <span style="font-size:20px"><a href="https://dszdsz.cn/">Shangzhe Di</a><sup>1</sup></span> </center>
        </td>
        <td align="center" width="125px">
          <center> <span style="font-size:20px">Zhonghua Zhai</a><sup>2</sup></span> </center>
        </td>
        <td align="center" width="115x">
          <center> <span style="font-size:20px">Weilin Huang</a><sup>2</sup></span> </center>
        </td>
        <td align="center" width="140x">
          <center> <span style="font-size:20px"><a href="https://cmic.sjtu.edu.cn/wangyanfeng/">Yanfeng Wang</a><sup>1 <img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span> </center>
        </td>
        <td align="center" width="120px">
          <center> <span style="font-size:20px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1 <img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span> </center>
        </td>
      </tr>
    </tbody>
  </table>
  
  <br>

  <table align="center" width="800px">
    <tbody>
      <tr>
        <td align="center" width="800px">
          <center> <span style="font-size:18px"><sup>1</sup>School of Artificial Intelligence, Shanghai Jiao Tong University</span> </center>
        </td>
        <td align="center" width="300px">
            <center> <span style="font-size:18px"><sup>2</sup>ByteDance</span> </center>
          </td>
      </tr>
    </tbody>
  </table>

  <br>

  <table align="center" width="800px">
    <tbody>
      <tr>
        <td align="center" width="800px">
          <center> <strong><span style="font-size:32px">Under Review</span></strong> </center>
        </td>
      </tr>
    </tbody>
  </table>

  <br>

  <table align="center" width="800px">
    <tbody>
      <tr>
        <td align="center" width="200px">
          <center> <span style="font-size:20px">Code <a href="https://github.com/Lzq5/UniTime"> [GitHub]</a> </span> </center>
        </td>

        <td align="center" width="200px">
          <center> <span style="font-size:20px"> Paper <a href="https://github.com/Lzq5/UniTime"> [arXiv]</a> </span> </center>
        </td>

        <td align="center" width="200px">
          <center> <span style="font-size:20px"> Data <a href="https://github.com/Lzq5/UniTime"> [HuggingFace]</a> </span> </center>
        </td>

        <td align="center" width="200px">
          <center> <span style="font-size:20px"> Cite <a href="./resources/bibtex.txt"> [BibTeX]</a> </span> </center>
        </td>
      </tr>
    </tbody>
  </table>
  
  <br>

  <hr>
  
  <br>

  <center> <img src="./resources/teaser.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a> </center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <left>
      The <strong style="font-weight: 900"> UniTime </strong> framework empowers MLLMs with advanced universal temporal grounding capabilities.
    (a) UniTime can handle diverse videos with various views, genres, and durations, as well as comprehend complex language queries.
    (b) UniTime achieves universal temporal grounding through a coarse-to-fine approach.
    (c) Performance comparison on temporal grounding and video question answering benchmarks demonstrates the superior capabilities of UniTime.
    </left>
  </p>

  <center> <h2> <strong> Abstract </strong> </h2> </center>
  <div class="abstract-container">
      <p style="text-align:justify; text-justify:inter-ideograph;">
        <left>
            This paper presents a computational model for universal video temporal grounding, which accurately localizes temporal moments in videos based on natural language queries (e.g., questions or descriptions). 
            Unlike existing methods that are often limited to specific video domains or durations, we propose <strong style="font-weight: 900"> UniTime </strong>, a robust and universal video grounding model leveraging the strong vision-language understanding capabilities of generative Multi-modal Large Language Models (MLLMs).
            Our model effectively handles videos of diverse views, genres, and lengths while comprehending complex language queries.
            The key contributions include:
            (i) We consider steering strong MLLMs for temporal grounding in videos. To enable precise timestamp outputs, we incorporate temporal information by interleaving timestamp tokens with video tokens.
            (ii) By training the model to handle videos with different input granularities through adaptive frame scaling, our approach achieves robust temporal grounding for both short and long videos.
            (iii) Comprehensive experiments show that UniTime outperforms state-of-the-art approaches in both zero-shot and dataset-specific finetuned settings across five public temporal grounding benchmarks.
            (iv) When employed as a preliminary moment retriever for long-form video question-answering (VideoQA), UniTime significantly improves VideoQA accuracy, highlighting its value for complex video understanding tasks.
        </left>
      </p>
  </div>

  <br>
  <hr>

  <center> <h2> <strong> UniTime Architecture </strong> </h2> </center>
  <center> <img class="center" src="./resources/arch.png" width="800px"></p> </center>

  <p style="text-align:justify; text-justify:inter-ideograph;">
    <left>
        Overview of the proposed <strong style="font-weight: 900"> UniTime </strong> framework. (a) Model{} achieves universal temporal grounding by leveraging adaptive frame scaling to construct multi-scale video inputs and then generate multi-scale predictions, allowing robust grounding across diverse video durations.
        (b) Within the model architecture, UniTime constructs an interleaved sequence of timestamps and scaled frame features, which, combined with the language query, is fed into the LLM and then identifies the corresponding temporal interval from the timestamp tokens.
    </left>
  </p>

  <br>
  <hr>

  <center> <h2> <strong> Results </strong> </h2> </center>
  <h3> <center> <strong> Quantitative Results </strong> </center> </h3>
  
  <center> <img class="center" src="./resources/ft.png" width="800px"></p> </center>
  <p>
    <left>
      <strong style="font-weight: 900"> Comparison with SoTA methods on video temporal grounding benchmarks. </strong>
        SP denotes the dataset-specific fine-tuning setting, and Full refers to the universal pre-training setting.
        Best results are in bold. Improvements of UniTime-Full over SoTA are highlighted in green.
    </left>
  </p>

  <br>
  
  <center> <img class="center" src="./resources/zs.png" width="800px"> </center>
  <p>
    <left>
      <strong style="font-weight: 900"> Zero-shot performance on video temporal grounding benchmarks. </strong>
        Zero indicates zero-shot. All models are evaluated in the zero-shot setting.
        Numbers in gray are sourced from the original paper; all others are tested by us using their released code and checkpoints.
        Improvements of UniTime-Zero over SoTA are highlighted in green.
    </left>
  </p>

  <br>
  
  <center> <img class="center" src="./resources/downstream.png" width="800px"> </center>
  <p>
    <left>
      <strong style="font-weight: 900"> Performance on VideoQA benchmarks. </strong>
      R@IoU is the mean Recall@1 at IoU thresholds [0.1:0.1:0.5]. For grounded VideoQA, IoP and mIoU are used as grounding metrics.
    </left>
  </p>

  <br>

  <h3> <center> <strong> Qualitative Results </strong> </center> </h3>
  <p> <img class="center" src="./resources/visualization.png" width="800px"> </p>
  <p>
    <left>
      <strong style="font-weight: 900">Qualitative Results for long-video temporal grounding on the Ego4D-NLQ benchmark. </strong>
      Red indicates the ground truth (GT), blue denotes the results of coarse-grained segment retrieval, and green represents the results of fine-grained temporal grounding.
    </left>
  </p>

  <br>

  <p> <center> <strong style="font-weight: 900; font-size: 18px">Welcome to check out our paper for more technical details and results!</strong> </center> </p>

  <hr>

  <center> <h2> <strong> Acknowledgements </strong> </h2> </center>
  <p> Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>. </p>

  <br>
  <br>

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>

</html>